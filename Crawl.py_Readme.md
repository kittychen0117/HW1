### Crawl.py Readme 

#### 使用套件：Requests、pandas、其餘皆為python內建
用Request的原因就是因為他很普遍，好操作，對於這次作業很適合
用Pandas的原因則是因為他最好拿來處理並分析資料

#### 程式邏輯(流程)
- 程式分成兩部份
- - 第一部分是先爬所有ETF，在三年份內的資料(Adj Close)，因為yahoo的資料可以透過url去抓取一段時間內的每日資料，所以我們要做的事情只要修改參數
並且用request.post得到資料即可
- - 第二部分是將爬好的資料整理成dataframe的形式，這地方比較tricky一點因為有些ETF有稍微缺了一些資料，所以必須將各個ETF的時間對好。

#### 執行方法
python3 crawl.py 即可

#### 遇到困難
- 其實各個etf有不同的主頁，這些主頁架構都不一樣，我認為要為每個不同類型的網頁都寫不一樣的爬蟲有點太不feasible，所以最後選擇爬yahoo
- 其實一開始是用selenium去爬，但是爬到最後需要捲動網頁時，發現yahoo的頁面在捲動上有些困難，因為他的頁面並不是像臉書的頁面可以無限捲動(這部分我有測試過)
一樣的code放到臉書上是可以無限捲動的，所以最後沒辦法抓下來，然後就是想試著用下載檔案的方式去爬，但是遇到的困難是svg這個tag怎麼抓都抓不太到，最後就換成
現在這個方式，然後selenium還有個問題是他是模仿真的網頁，所以網頁載入時間都不太一樣，如果沒有sleep的話可能會有時候程式可以跑有時候不行。
- 另外一個問題是pandas的dataframe其實我不太熟悉，所以在合併所有的ETF資料時遇到了蠻多困難的，時間對不上啊，資料數量不一樣啊，等等
